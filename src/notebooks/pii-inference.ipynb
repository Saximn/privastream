{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7738063,"sourceType":"datasetVersion","datasetId":4522540},{"sourceId":7739394,"sourceType":"datasetVersion","datasetId":4523475},{"sourceId":8141711,"sourceType":"datasetVersion","datasetId":4809875},{"sourceId":8147235,"sourceType":"datasetVersion","datasetId":4358264},{"sourceId":8165328,"sourceType":"datasetVersion","datasetId":4830220},{"sourceId":8182470,"sourceType":"datasetVersion","datasetId":4843256,"isSourceIdPinned":true}],"dockerImageVersionId":30636,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp /kaggle/input/piidd-postprocessing-code-dataset/piidd_postprocessing.py .\n!ls .","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-25T12:52:07.114782Z","iopub.execute_input":"2024-04-25T12:52:07.11516Z","iopub.status.idle":"2024-04-25T12:52:09.042322Z","shell.execute_reply.started":"2024-04-25T12:52:07.11513Z","shell.execute_reply":"2024-04-25T12:52:09.041074Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile custom_model_23.py\n\nfrom transformers import AutoTokenizer,AutoConfig, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom transformers import DebertaV2Config, DebertaV2ForTokenClassification\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\nfrom transformers.utils import PaddingStrategy\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback\nfrom datasets import Dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom torch import nn\nimport torch\nfrom transformers import Trainer\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nfrom transformers.models.deberta.modeling_deberta import (\n    DebertaPreTrainedModel,\n    DebertaModel\n)\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import (\n    DebertaV2Model,DebertaV2PreTrainedModel\n)\n## Pooling Strategies\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\nclass MaxPooling(nn.Module):\n    def __init__(self):\n        super(MaxPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = -1e4\n        max_embeddings, _ = torch.max(embeddings, dim=1)\n        return max_embeddings\n\nclass MinPooling(nn.Module):\n    def __init__(self):\n        super(MinPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = 1e4\n        min_embeddings, _ = torch.min(embeddings, dim=1)\n        return min_embeddings\n# v2 vor latest\nclass CustomModel(DebertaV2PreTrainedModel):#nn.Module):\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    #https://github.com/huggingface/transformers/blob/f7ef7cec6c6c162087421f36a17eabdbb223579d/src/transformers/models/deberta/modeling_deberta.py#L1342\n    #def __init__(self,backbone,bilstm_layer=True,class_weights=None):\n    def __init__(self, config):\n        #super(CustomModel, self).__init__(config)\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.deberta = DebertaV2Model(config)\n        print(f\"Num Labels {config.num_labels}\")\n        self.mean_pooling = MeanPooling()\n        # self.max_pooler = MaxPooling()\n        # self.min_pooler = MinPooling()\n        self.bilstm_layer = False\n        self.mult_sample_dpt = True\n        self.mean_pool = False\n        # Loss Fn\n        o_weight=0.05\n        self.class_weights = torch.tensor([1.0]*(self.num_labels - 1) + [o_weight])\n        self.loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n        print(f\"Class Weights {self.class_weights}\")\n\n        if self.bilstm_layer:\n            print(f'Including LSTM layer hidden size {self.config.hidden_size} dropout {self.config.hidden_dropout_prob}')\n            self.lstm = nn.LSTM(config.hidden_size, (config.hidden_size) // 2, num_layers=2, dropout=config.hidden_dropout_prob, batch_first=True,\n                              bidirectional=True)\n            self.initialize_lstm(self.lstm)\n\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        #self._init_weights\n        self.post_init()\n    def initialize_lstm(self, lstm_layer):\n        for name, param in lstm_layer.named_parameters():\n            if 'weight_ih' in name:\n                torch.nn.init.xavier_uniform_(param.data)\n            elif 'weight_hh' in name:\n                torch.nn.init.orthogonal_(param.data)\n            elif 'bias' in name:\n                param.data.fill_(0)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def get_input_embeddings(self):\n        return self.deberta.get_input_embeddings()\n\n    def set_input_embeddings(self, new_embeddings):\n        self.deberta.set_input_embeddings(new_embeddings)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.deberta(\n            input_ids,\n            token_type_ids=token_type_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        output_backbone = outputs[0]\n        if self.bilstm_layer:\n            output_backbone = self.dropout(output_backbone)\n            output, hc = self.lstm(output_backbone)\n\n        if self.mean_pool:\n            output_backbone = self.dropout(output_backbone)\n            output = self.mean_pooling(output_backbone, attention_mask)\n        # max_pool = self.max_pooler(output, attention_mask)\n        # min_pool = self.min_pooler(output, attention_mask)\n        #concat = torch.cat([mean_pool], dim=1)\n\n        # Multi-sample dropout.\n        if self.mult_sample_dpt:\n            output1 = self.classifier(self.dropout1(output_backbone))\n            output2 = self.classifier(self.dropout2(output_backbone))\n            output3 = self.classifier(self.dropout3(output_backbone))\n            output4 = self.classifier(self.dropout4(output_backbone))\n            output5 = self.classifier(self.dropout5(output_backbone))\n            logits = (output1 + output2 + output3 + output4 + output5) / 5\n        else:\n            logits = self.classifier(output)\n\n        loss = None\n\n        if labels is not None:\n            if self.mean_pool:\n                loss = self.loss_fct(logits, labels.view(-1))\n            else:\n                loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:52:09.045014Z","iopub.execute_input":"2024-04-25T12:52:09.045409Z","iopub.status.idle":"2024-04-25T12:52:09.061955Z","shell.execute_reply.started":"2024-04-25T12:52:09.045364Z","shell.execute_reply":"2024-04-25T12:52:09.060889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile custom_model_bilstm.py\n\nfrom transformers import AutoTokenizer,AutoConfig, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom transformers import DebertaV2Config, DebertaV2ForTokenClassification\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\nfrom transformers.utils import PaddingStrategy\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback\nfrom datasets import Dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom torch import nn\nimport torch\nfrom transformers import Trainer\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nfrom transformers.models.deberta.modeling_deberta import (\n    DebertaPreTrainedModel,\n    DebertaModel\n)\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import (\n    DebertaV2Model,DebertaV2PreTrainedModel\n)\n## Pooling Strategies\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\nclass MaxPooling(nn.Module):\n    def __init__(self):\n        super(MaxPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = -1e4\n        max_embeddings, _ = torch.max(embeddings, dim=1)\n        return max_embeddings\n\nclass MinPooling(nn.Module):\n    def __init__(self):\n        super(MinPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = 1e4\n        min_embeddings, _ = torch.min(embeddings, dim=1)\n        return min_embeddings\n# v2 vor latest\nclass CustomModel(DebertaV2PreTrainedModel):#nn.Module):\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    #https://github.com/huggingface/transformers/blob/f7ef7cec6c6c162087421f36a17eabdbb223579d/src/transformers/models/deberta/modeling_deberta.py#L1342\n    def __init__(self, config):\n        #super(CustomModel, self).__init__(config)\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.deberta = DebertaV2Model(config)\n        print(f\"Num Labels {config.num_labels}\")\n        self.bilstm_layer = True\n        self.mult_sample_dpt = False\n        # Loss Fn\n        o_weight=0.05\n        self.loss_fct = torch.nn.CrossEntropyLoss()#weight=self.class_weights)\n        #print(f\"Class Weights {self.class_weights}\")\n\n        if self.bilstm_layer:\n            print(f'Including LSTM layer hidden size {self.config.hidden_size} dropout {self.config.hidden_dropout_prob}')\n            self.bilstm = nn.LSTM(config.hidden_size, (config.hidden_size) // 2, num_layers=1, dropout=config.hidden_dropout_prob, batch_first=True,\n                              bidirectional=True)\n            #self.initialize_lstm(self.bilstm)\n\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        #self._init_weights\n        self.post_init()\n    def initialize_lstm(self, lstm_layer):\n        for name, param in lstm_layer.named_parameters():\n            if 'weight_ih' in name:\n                torch.nn.init.xavier_uniform_(param.data)\n            elif 'weight_hh' in name:\n                torch.nn.init.orthogonal_(param.data)\n            elif 'bias' in name:\n                param.data.fill_(0)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def get_input_embeddings(self):\n        return self.deberta.get_input_embeddings()\n\n    def set_input_embeddings(self, new_embeddings):\n        self.deberta.set_input_embeddings(new_embeddings)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.deberta(\n            input_ids,\n            token_type_ids=token_type_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        output_backbone = outputs[0]\n        if self.bilstm_layer:\n            output_backbone = self.dropout(output_backbone)\n            #print(\"output_backbone \",output_backbone)\n            self.bilstm.flatten_parameters()\n            output, hc = self.bilstm(output_backbone)\n            #print(\"lstm output \",output)\n \n        # Multi-sample dropout.\n        if self.mult_sample_dpt:\n            output1 = self.classifier(self.dropout1(output_backbone))\n            output2 = self.classifier(self.dropout2(output_backbone))\n            output3 = self.classifier(self.dropout3(output_backbone))\n            output4 = self.classifier(self.dropout4(output_backbone))\n            output5 = self.classifier(self.dropout5(output_backbone))\n            logits = (output1 + output2 + output3 + output4 + output5) / 5\n        else:\n            logits = self.classifier(output)\n\n        loss = None\n\n        if labels is not None:\n            if self.mean_pool:\n                loss = self.loss_fct(logits, labels.view(-1))\n            else:\n                loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:52:09.063502Z","iopub.execute_input":"2024-04-25T12:52:09.063878Z","iopub.status.idle":"2024-04-25T12:52:09.079563Z","shell.execute_reply.started":"2024-04-25T12:52:09.063826Z","shell.execute_reply":"2024-04-25T12:52:09.078589Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile custom_model_distil.py\n\nfrom transformers import AutoTokenizer,AutoConfig, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom transformers import DebertaV2Config, DebertaV2ForTokenClassification\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\nfrom transformers.utils import PaddingStrategy\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback\nfrom datasets import Dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom torch import nn\nimport torch\nfrom transformers import Trainer\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nfrom transformers.models.deberta.modeling_deberta import (\n    DebertaPreTrainedModel,\n    DebertaModel\n)\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import (\n    DebertaV2Model,DebertaV2PreTrainedModel\n)\n## Pooling Strategies\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\nclass MaxPooling(nn.Module):\n    def __init__(self):\n        super(MaxPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = -1e4\n        max_embeddings, _ = torch.max(embeddings, dim=1)\n        return max_embeddings\n\nclass MinPooling(nn.Module):\n    def __init__(self):\n        super(MinPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = 1e4\n        min_embeddings, _ = torch.min(embeddings, dim=1)\n        return min_embeddings\n# v2 vor latest\nclass CustomModel(DebertaV2PreTrainedModel):#nn.Module):\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    #https://github.com/huggingface/transformers/blob/f7ef7cec6c6c162087421f36a17eabdbb223579d/src/transformers/models/deberta/modeling_deberta.py#L1342\n    #def __init__(self,backbone,bilstm_layer=True,class_weights=None):\n    def __init__(self, config):\n        #super(CustomModel, self).__init__(config)\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.deberta = DebertaV2Model(config)\n        print(f\"Num Labels {config.num_labels}\")\n        self.mean_pooling = MeanPooling()\n        # self.max_pooler = MaxPooling()\n        # self.min_pooler = MinPooling()\n        self.bilstm_layer = False\n        self.mult_sample_dpt = True\n        self.mean_pool = False\n        # Loss Fn\n        o_weight=0.05\n        self.class_weights = torch.tensor([1.0]*(self.num_labels-1) + [0.05])\n        self.loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n        #print(f\"Class Weights {self.class_weights}\")\n\n        if self.bilstm_layer:\n            print(f'Including LSTM layer hidden size {self.config.hidden_size} dropout {self.config.hidden_dropout_prob}')\n            self.lstm = nn.LSTM(config.hidden_size, (config.hidden_size) // 2, num_layers=2, dropout=config.hidden_dropout_prob, batch_first=True,\n                              bidirectional=True)\n            self.initialize_lstm(self.lstm)\n\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        #self._init_weights\n        self.post_init()\n    def initialize_lstm(self, lstm_layer):\n        for name, param in lstm_layer.named_parameters():\n            if 'weight_ih' in name:\n                torch.nn.init.xavier_uniform_(param.data)\n            elif 'weight_hh' in name:\n                torch.nn.init.orthogonal_(param.data)\n            elif 'bias' in name:\n                param.data.fill_(0)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def get_input_embeddings(self):\n        return self.deberta.get_input_embeddings()\n\n    def set_input_embeddings(self, new_embeddings):\n        self.deberta.set_input_embeddings(new_embeddings)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.deberta(\n            input_ids,\n            token_type_ids=token_type_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        output_backbone = outputs[0]\n        if self.bilstm_layer:\n            output_backbone = self.dropout(output_backbone)\n            output, hc = self.lstm(output_backbone)\n\n        if self.mean_pool:\n            output_backbone = self.dropout(output_backbone)\n            output = self.mean_pooling(output_backbone, attention_mask)\n        # max_pool = self.max_pooler(output, attention_mask)\n        # min_pool = self.min_pooler(output, attention_mask)\n        #concat = torch.cat([mean_pool], dim=1)\n\n        # Multi-sample dropout.\n        if self.mult_sample_dpt:\n            output1 = self.classifier(self.dropout1(output_backbone))\n            output2 = self.classifier(self.dropout2(output_backbone))\n            output3 = self.classifier(self.dropout3(output_backbone))\n            output4 = self.classifier(self.dropout4(output_backbone))\n            output5 = self.classifier(self.dropout5(output_backbone))\n            logits = (output1 + output2 + output3 + output4 + output5) / 5\n        else:\n            logits = self.classifier(output)\n\n        loss = None\n\n        if labels is not None:\n            if self.mean_pool:\n                loss = self.loss_fct(logits, labels.view(-1))\n            else:\n                loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:52:09.082014Z","iopub.execute_input":"2024-04-25T12:52:09.082315Z","iopub.status.idle":"2024-04-25T12:52:09.098338Z","shell.execute_reply.started":"2024-04-25T12:52:09.082291Z","shell.execute_reply":"2024-04-25T12:52:09.096962Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport argparse\nfrom pathlib import Path\nfrom itertools import chain\nimport re\n\nimport numpy as np\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom datasets import Dataset\nfrom scipy.special import softmax\nfrom collections import defaultdict\nfrom typing import Dict\nfrom piidd_postprocessing import label_postprocessing\n\nclass PRFScore:\n    \"\"\"A precision / recall / F score.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        tp: int = 0,\n        fp: int = 0,\n        fn: int = 0,\n    ) -> None:\n        self.tp = tp\n        self.fp = fp\n        self.fn = fn\n\n    def __len__(self) -> int:\n        return self.tp + self.fp + self.fn\n\n    def __iadd__(self, other):  # in-place add\n        self.tp += other.tp\n        self.fp += other.fp\n        self.fn += other.fn\n        return self\n\n    def __add__(self, other):\n        return PRFScore(\n            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n        )\n\n    def score_set(self, cand: set, gold: set) -> None:\n        self.tp += len(cand.intersection(gold))\n        self.fp += len(cand - gold)\n        self.fn += len(gold - cand)\n\n    @property\n    def precision(self) -> float:\n        return self.tp / (self.tp + self.fp + 1e-100)\n\n    @property\n    def recall(self) -> float:\n        return self.tp / (self.tp + self.fn + 1e-100)\n\n    @property\n    def f1(self) -> float:\n        p = self.precision\n        r = self.recall\n        return 2 * ((p * r) / (p + r + 1e-100))\n\n    @property\n    def f5(self) -> float:\n        beta = 5\n        p = self.precision\n        r = self.recall\n\n        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n        return fbeta\n\n    def to_dict(self) -> Dict[str, float]:\n        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n\n\ndef compute_metrics(pred_df, gt_df):\n    \"\"\"\n    Compute the LB metric (lb) and other auxiliary metrics\n    \"\"\"\n    \n    references = {(row.document, row.token, row.label) for row in gt_df.itertuples()}\n    predictions = {(row.document, row.token, row.label) for row in pred_df.itertuples()}\n\n    score_per_type = defaultdict(PRFScore)\n    references = set(references)\n\n    for ex in predictions:\n        pred_type = ex[-1] # (document, token, label)\n        if pred_type != 'O':\n            pred_type = pred_type[2:] # avoid B- and I- prefix\n            \n        if pred_type not in score_per_type:\n            score_per_type[pred_type] = PRFScore()\n\n        if ex in references:\n            score_per_type[pred_type].tp += 1\n            references.remove(ex)\n        else:\n            score_per_type[pred_type].fp += 1\n\n    for doc, tok, ref_type in references:\n        if ref_type != 'O':\n            ref_type = ref_type[2:] # avoid B- and I- prefix\n        \n        if ref_type not in score_per_type:\n            score_per_type[ref_type] = PRFScore()\n        score_per_type[ref_type].fn += 1\n\n    totals = PRFScore()\n    \n    for prf in score_per_type.values():\n        totals += prf\n\n    return {\n        \"ents_p\": totals.precision,\n        \"ents_r\": totals.recall,\n        \"ents_f5\": totals.f5,\n        \"ents_per_type\": {k: v.to_dict() for k, v in score_per_type.items() if k!= 'O'},\n    }\n\ndef tokenize(example, tokenizer):\n    text = []\n    token_map = []\n    \n    idx = 0\n    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        text.append(t)\n        token_map.extend([idx]*len(t))\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n        idx += 1\n        \n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=False)\n    return {\n        **tokenized,\n        \"token_map\": token_map,\n    }\n\ndef parse_predictions(pred_softmax, ds, id2label, thresholds):\n    preds = pred_softmax.argmax(-1)\n    preds_without_O = pred_softmax[:,:,:len(id2label)-1].argmax(-1)\n    O_preds = pred_softmax[:,:,len(id2label)-1]\n    \n    indexes = defaultdict(list)\n    for k,v in id2label.items():\n        if k != str(len(id2label)-1):\n            indexes[v.split(\"-\")[1]].append(int(k))\n\n    print(indexes)\n\n    for label_name, label_threshold in thresholds.items():\n        if len(indexes[label_name]) == 1:\n            preds = np.where(O_preds < label_threshold, \n                             np.where(preds_without_O == indexes[label_name][0], preds_without_O, preds), \n                             preds)\n        else:\n            preds = np.where(O_preds < label_threshold, \n                             np.where((preds_without_O == indexes[label_name][0]) | (preds_without_O == indexes[label_name][1]), preds_without_O, preds), \n                             preds)\n\n    preds_final = preds\n\n    \n    pairs = set()\n    document, token, label, token_str = [], [], [], []\n    for p, token_map, offsets, tokens, doc in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[str(token_pred)]\n\n            if start_idx + end_idx == 0: \n                continue\n\n            if token_map[start_idx] == -1:\n                start_idx += 1\n\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n\n            if start_idx >= len(token_map): \n                break\n\n            token_id = token_map[start_idx]\n            if label_pred in (\"O\") or token_id == -1:\n                continue\n\n            pair = (doc, token_id)\n            if pair in pairs:\n                continue\n\n            document.append(doc)\n            token.append(token_id)\n            label.append(label_pred)\n            token_str.append(tokens[token_id])\n            pairs.add(pair)\n            \n    df = pd.DataFrame({\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n    df[\"row_id\"] = list(range(len(df)))\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:52:09.100051Z","iopub.execute_input":"2024-04-25T12:52:09.100658Z","iopub.status.idle":"2024-04-25T12:52:30.610349Z","shell.execute_reply.started":"2024-04-25T12:52:09.100624Z","shell.execute_reply":"2024-04-25T12:52:30.609517Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weights = {\n    'custom-model-23': 2.3136772671200951 ,\n    'custom-model-distil': 8.7434000461426,\n    'exp073': 4.958400473808574,\n    'custom-model-bilstm': 2.0696284831356238,\n    'exp076': 4.595871408325738,\n    'basic-model-2': 9.942166951500617, #deberta large\n    'custom-model-2': 2.5511467213118255, #deberta large\n}\n\nvoting_thr = 14.720990487155191\n\nmodels_paths = [\n    (['/kaggle/input/pii-detection-models/custom-model-23-fullfit-seed42/custom-model-23-fullfit-seed42'], 'custom-model-23', \n     {'EMAIL': 0.5, 'ID_NUM': 0.6, 'NAME_STUDENT': 0.8, 'PHONE_NUM': 0.5, 'STREET_ADDRESS': 0.5, 'URL_PERSONAL': 0.5, 'USERNAME': 0.8}),\n    \n    (['/kaggle/input/distil-mpware-fp16-maxlen2048-lr7e-5/fold0-cv9758', '/kaggle/input/distil-mpware-fp16-maxlen2048-lr7e-5/fold1-cv9687'], 'custom-model-distil', \n     {'EMAIL': 0.5, 'ID_NUM': 0.6, 'NAME_STUDENT': 0.8, 'PHONE_NUM': 0.5, 'STREET_ADDRESS': 0.5, 'URL_PERSONAL': 0.5, 'USERNAME': 0.8}),\n    \n    (['/kaggle/input/pii-detect-exp073-0'], 'exp073', \n     {'EMAIL': 0.5, 'ID_NUM': 0.9, 'NAME_STUDENT': 0.9, 'PHONE_NUM': 0.6, 'STREET_ADDRESS': 0.6, 'URL_PERSONAL': 0.5, 'USERNAME': 0.9}),\n    \n    (['/kaggle/input/bilstm1-nosafe-maxlen1650/fold3-lr7e-5-cv9709'], 'custom-model-bilstm', \n     {'EMAIL': 0.5, 'ID_NUM': 0.6, 'NAME_STUDENT': 0.8, 'PHONE_NUM': 0.5, 'STREET_ADDRESS': 0.5, 'URL_PERSONAL': 0.5, 'USERNAME': 0.8}),\n    \n    (['/kaggle/input/pii-detect-exp076-0'], 'exp076', \n     {'EMAIL': 0.5, 'ID_NUM': 0.9, 'NAME_STUDENT': 0.9, 'PHONE_NUM': 0.6, 'STREET_ADDRESS': 0.6, 'URL_PERSONAL': 0.5, 'USERNAME': 0.9}), \n    \n    (\n        [\n            '/kaggle/input/pii-detection-models/basic-model-2-fullfit-seed42/basic-model-2-fullfit-seed42', \n            '/kaggle/input/pii-detection-models/basic-model-2-fullfit-seed6543/basic-model-2-fullfit-seed6543',\n        ], \n        'basic-model-2',\n        {\n            'EMAIL': 0.5,\n            'ID_NUM': 0.6,\n            'NAME_STUDENT': 0.8,\n            'PHONE_NUM': 0.5,\n            'STREET_ADDRESS': 0.5,\n            'URL_PERSONAL': 0.5,\n            'USERNAME': 0.8,\n        }\n        \n    ),\n    (\n        [\n            '/kaggle/input/pii-detection-models/custom-model-2-fullfit-seed42/custom-model-2-fullfit-seed42',\n            '/kaggle/input/pii-detection-models/custom-model-2-fullfit-seed6543/custom-model-2-fullfit-seed6543',\n        ], \n        'custom-model-2',\n        {\n            'EMAIL': 0.5,\n            'ID_NUM': 0.6,\n            'NAME_STUDENT': 0.8,\n            'PHONE_NUM': 0.5,\n            'STREET_ADDRESS': 0.5,\n            'URL_PERSONAL': 0.5,\n            'USERNAME': 0.8,\n        }\n    ),\n]\n\nwith open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\", \"r\") as f:\n    data = json.load(f)\n    \ndoc2tokens = {str(row[\"document\"]): row[\"tokens\"] for row in data}\n    \ndfs = []\nfor group_paths, group_name, thresholds in models_paths:\n    print(f\"creating predictions for {group_name}\")\n    ds = Dataset.from_dict({\n        \"full_text\": [x[\"full_text\"] for x in data],\n        \"document\": [x[\"document\"] for x in data],\n        \"tokens\": [x[\"tokens\"] for x in data],\n        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n    })\n    tokenizer = AutoTokenizer.from_pretrained(group_paths[0])\n    collator = DataCollatorForTokenClassification(tokenizer)\n    ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=2)\n\n    config = json.load(open(f'/kaggle/input/pii-detection-models/basic-model-2-fullfit-seed42/basic-model-2-fullfit-seed42/config.json'))\n    id2label = config[\"id2label\"]\n    print(id2label)\n    \n    weighted_average_predictions = None\n    for model_path in group_paths:\n        \n        if \"custom\" in group_name:\n            if \"distil\" in group_name:\n                from custom_model_distil import CustomModel\n                print(\"initializing distil model\")\n            elif \"bilstm\" in group_name:\n                from custom_model_bilstm import CustomModel\n                print(\"initializing bilstm model\")\n            else:\n                from custom_model_23 import CustomModel\n                print(\"initializing custom model\")\n            \n            backbone = CustomModel.from_pretrained(model_path)\n            backbone.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=16)\n            model = backbone\n        else:\n            model = AutoModelForTokenClassification.from_pretrained(model_path)\n        args = TrainingArguments(\n            \".\", \n            per_device_eval_batch_size=1, \n            report_to=\"none\",\n        )\n        trainer = Trainer(\n            model=model, \n            args=args, \n            data_collator=collator, \n            tokenizer=tokenizer,\n        )\n        predictions = trainer.predict(ds).predictions\n        if weighted_average_predictions is None:\n            weighted_average_predictions = softmax(predictions, axis = -1) * (1 / len(group_paths))\n        else:\n            weighted_average_predictions += softmax(predictions, axis = -1) * (1 / len(group_paths))\n    \n    df = parse_predictions(weighted_average_predictions, ds, id2label, thresholds)\n    df['weight'] = weights[group_name]\n    dfs.append(df)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:52:30.611644Z","iopub.execute_input":"2024-04-25T12:52:30.612252Z","iopub.status.idle":"2024-04-25T12:55:42.948706Z","shell.execute_reply.started":"2024-04-25T12:52:30.612221Z","shell.execute_reply":"2024-04-25T12:55:42.94761Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.concat(dfs)\ndf = df.groupby(['document', 'token', 'label', 'token_str'])['weight'].sum().reset_index()\nprint(\"weighted df\")\ndisplay(df)\ndf = df[df['weight'] >= voting_thr]\n\ndf = df.sort_values(['document', 'token', 'weight'], ascending=[True, True, False])\ndf = df.drop_duplicates(['document', 'token'], keep='first')\n\ntrain_data = json.load(open('/kaggle/input/pii-detection-removal-from-educational-data/test.json'))\ndfs = []\nfor doc in df['document'].unique():\n    sub = df[df['document'] == doc].copy()\n    if not 'I-NAME_STUDENT' in sub['label'].values:\n        dfs.append(sub)\n        continue\n    for sample in train_data:\n        if sample['document'] == doc:\n            break\n    new_labels = []\n    for tok, lab in sub[['token', 'label']].values:\n        if lab == 'I-NAME_STUDENT' and '\\n' in sample['tokens'][tok-1]:\n            new_labels.append('B-NAME_STUDENT')\n        else:\n            new_labels.append(lab)\n    sub['label'] = new_labels\n    dfs.append(sub)       \ndf = pd.concat(dfs).drop(columns=[\"weight\"])\n\ndf","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:55:42.950104Z","iopub.execute_input":"2024-04-25T12:55:42.950421Z","iopub.status.idle":"2024-04-25T12:55:43.023722Z","shell.execute_reply.started":"2024-04-25T12:55:42.950391Z","shell.execute_reply":"2024-04-25T12:55:43.022728Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LABEL BASED POSTPROCESSING","metadata":{}},{"cell_type":"code","source":"data = json.load(open('/kaggle/input/pii-detection-removal-from-educational-data/test.json'))\ndf = label_postprocessing(df, doc2tokens, data)\ndisplay(df)\ndf[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:55:43.024808Z","iopub.execute_input":"2024-04-25T12:55:43.025116Z","iopub.status.idle":"2024-04-25T12:55:43.125109Z","shell.execute_reply.started":"2024-04-25T12:55:43.025091Z","shell.execute_reply":"2024-04-25T12:55:43.124231Z"},"trusted":true},"outputs":[],"execution_count":null}]}