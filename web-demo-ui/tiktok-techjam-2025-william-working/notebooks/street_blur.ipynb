{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d546fa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileExistsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mlink_or_copy\u001b[39m\u001b[34m(src, dst)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# hardlink (fast, no extra space if same drive)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[31mFileExistsError\u001b[39m: [WinError 183] Cannot create a file when that file already exists: '..\\\\datasets\\\\ICPR\\\\train\\\\img_1.jpg' -> '..\\\\datasets\\\\ICPR\\\\icdar15_std\\\\ch4_training_images\\\\img_1.jpg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m (ROOT/\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m).glob(\u001b[33m\"\u001b[39m\u001b[33m*.jpg\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mlink_or_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTD\u001b[49m\u001b[43m/\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mch4_training_images\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m/\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m (ROOT/\u001b[33m\"\u001b[39m\u001b[33mtrain_truth\u001b[39m\u001b[33m\"\u001b[39m).glob(\u001b[33m\"\u001b[39m\u001b[33mgt_*.txt\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     22\u001b[39m     link_or_copy(p, STD/\u001b[33m\"\u001b[39m\u001b[33mch4_training_localization_transcription_gt\u001b[39m\u001b[33m\"\u001b[39m/p.name)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mlink_or_copy\u001b[39m\u001b[34m(src, dst)\u001b[39m\n\u001b[32m     14\u001b[39m     os.link(src, dst)  \u001b[38;5;66;03m# hardlink (fast, no extra space if same drive)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy2\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\shutil.py:453\u001b[39m, in \u001b[36mcopy2\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    451\u001b[39m     flags |= _winapi.COPY_FILE_COPY_SYMLINK\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m     \u001b[43m_winapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCopyFile2\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dst\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[31mPermissionError\u001b[39m: [WinError 32] The process cannot access the file because it is being used by another process"
     ]
    }
   ],
   "source": [
    "# run once in a notebook cell from repo root\n",
    "from pathlib import Path\n",
    "import shutil, os\n",
    "\n",
    "ROOT = Path(\"../datasets/ICPR\")\n",
    "STD  = ROOT / \"icdar15_std\"\n",
    "(STD / \"ch4_training_images\").mkdir(parents=True, exist_ok=True)\n",
    "(STD / \"ch4_training_localization_transcription_gt\").mkdir(parents=True, exist_ok=True)\n",
    "(STD / \"ch4_test_images\").mkdir(parents=True, exist_ok=True)\n",
    "(STD / \"ch4_test_localization_transcription_gt\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def link_or_copy(src, dst):\n",
    "    try:\n",
    "        os.link(src, dst)  # hardlink (fast, no extra space if same drive)\n",
    "    except Exception:\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "# train\n",
    "for p in (ROOT/\"train\").glob(\"*.jpg\"):\n",
    "    link_or_copy(p, STD/\"ch4_training_images\"/p.name)\n",
    "for p in (ROOT/\"train_truth\").glob(\"gt_*.txt\"):\n",
    "    link_or_copy(p, STD/\"ch4_training_localization_transcription_gt\"/p.name)\n",
    "\n",
    "# test\n",
    "for p in (ROOT/\"test\").glob(\"*.jpg\"):\n",
    "    link_or_copy(p, STD/\"ch4_test_images\"/p.name)\n",
    "for p in (ROOT/\"test_truth\").glob(\"gt_*.txt\"):\n",
    "    link_or_copy(p, STD/\"ch4_test_localization_transcription_gt\"/p.name)\n",
    "\n",
    "print(\"ICDAR15 standardized at:\", STD.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f6884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train imgs: 1000\n",
      "train gts : 1000\n",
      "test  imgs: 500\n",
      "test  gts : 500\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "STD = Path(\"../datasets/ICPR/icdar15_std\")\n",
    "print(\"train imgs:\", len(list((STD/\"ch4_training_images\").glob(\"*.jpg\"))))\n",
    "print(\"train gts :\", len(list((STD/\"ch4_training_localization_transcription_gt\").glob(\"gt_*.txt\"))))\n",
    "print(\"test  imgs:\", len(list((STD/\"ch4_test_images\").glob(\"*.jpg\"))))\n",
    "print(\"test  gts :\", len(list((STD/\"ch4_test_localization_transcription_gt\").glob(\"gt_*.txt\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b7c07",
   "metadata": {},
   "source": [
    "# Optional baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40dde3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os, re, time, json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32bee053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR backend ready.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class OCRPipeline:\n",
    "    def __init__(self, det_arch=\"db_resnet50\", reco_arch=\"parseq\"):\n",
    "        try:\n",
    "            from doctr.models import ocr_predictor\n",
    "            self.kind = \"doctr\"\n",
    "            self.model = ocr_predictor(det_arch=det_arch, reco_arch=reco_arch, pretrained=True)\n",
    "            self.model = self.model.to(device).eval()\n",
    "        except Exception as e:\n",
    "            print(\"docTR unavailable → falling back to EasyOCR:\", e)\n",
    "            import easyocr\n",
    "            self.kind = \"easyocr\"\n",
    "            self.reader = easyocr.Reader([\"en\"], gpu=torch.cuda.is_available())\n",
    "\n",
    "    def infer(self, img_bgr) -> Dict:\n",
    "        \"\"\"\n",
    "        Returns a normalized export dict like docTR's .export() so downstream code is uniform.\n",
    "        \"\"\"\n",
    "        if self.kind == \"doctr\":\n",
    "            rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "            doc = self.model([rgb])\n",
    "            return doc.export()   # {pages:[{blocks:[{lines:[{words:[{value, confidence, geometry}]}]}]}]}\n",
    "        else:\n",
    "            # emulate a minimal export with EasyOCR\n",
    "            res = self.reader.readtext(img_bgr, detail=1, paragraph=False)\n",
    "            H, W = img_bgr.shape[:2]\n",
    "            words = []\n",
    "            for box, text, conf in res:\n",
    "                xs = [p[0]/W for p in box]; ys = [p[1]/H for p in box]\n",
    "                x0, x1 = min(xs), max(xs); y0, y1 = min(ys), max(ys)\n",
    "                words.append({\"value\": text, \"confidence\": float(conf), \"geometry\": ((x0,y0),(x1,y1))})\n",
    "            return {\"pages\":[{\"blocks\":[{\"lines\":[{\"words\":words, \"geometry\": ((0,0),(1,1))}]}]}]}\n",
    "            \n",
    "ocr = OCRPipeline(det_arch=\"db_resnet50\", reco_arch=\"parseq\")\n",
    "print(\"OCR backend ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1849a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base tokens (extend per locale)\n",
    "STREET_TOKENS = r\"(Street|St|Road|Rd|Avenue|Ave|Drive|Dr|Lane|Ln|Boulevard|Blvd|Way|Terrace|Ter|Court|Ct|Crescent|Cres|Place|Pl|Highway|Hwy|Expressway|Expwy|Jalan|Jln|Lorong|Lor)\"\n",
    "UNIT          = r\"#\\s?\\d{1,3}-\\d{1,4}\"\n",
    "POSTAL_SG     = r\"\\b(?:S\\s*)?\\d{6}\\b\"\n",
    "HOUSE_NO      = r\"\\b(?:Blk|Block)?\\s?\\d{1,5}[A-Z]?\\b\"\n",
    "COMPOSED      = rf\"{HOUSE_NO}.*\\b{STREET_TOKENS}\\b\"\n",
    "\n",
    "PATTERNS = [STREET_TOKENS, UNIT, POSTAL_SG, COMPOSED]\n",
    "REGEXES  = [re.compile(p, re.IGNORECASE) for p in PATTERNS]\n",
    "\n",
    "# Optional: load extra street names from a text file (one per line)\n",
    "EXTRA_STREETS_FILE = None  # e.g., \"./runs_sg/street_tokens.txt\"\n",
    "if EXTRA_STREETS_FILE and os.path.exists(EXTRA_STREETS_FILE):\n",
    "    with open(EXTRA_STREETS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        names = [re.escape(l.strip()) for l in f if l.strip()]\n",
    "    if names:\n",
    "        REGEXES.append(re.compile(r\"(\" + \"|\".join(names) + r\")\", re.IGNORECASE))\n",
    "\n",
    "def is_pii_text(text: str, conf: float, conf_thresh: float = 0.35) -> bool:\n",
    "    \"\"\"Rules only; no ML, no fine-tuning.\"\"\"\n",
    "    if not text or conf < conf_thresh:\n",
    "        return False\n",
    "    t = text.strip()\n",
    "    return any(rx.search(t) for rx in REGEXES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12ba6659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_from_box_norm(box, W, H):\n",
    "    (x0,y0),(x1,y1) = box\n",
    "    x0, x1 = int(x0*W), int(x1*W)\n",
    "    y0, y1 = int(y0*H), int(y1*H)\n",
    "    return np.array([[x0,y0],[x1,y0],[x1,y1],[x0,y1]], dtype=np.int32)\n",
    "\n",
    "def iou(a, b):\n",
    "    xi1, yi1 = max(a[0],b[0]), max(a[1],b[1])\n",
    "    xi2, yi2 = min(a[2],b[2]), min(a[3],b[3])\n",
    "    inter = max(0, xi2-xi1) * max(0, yi2-yi1)\n",
    "    if inter <= 0: return 0.0\n",
    "    area_a = (a[2]-a[0])*(a[3]-a[1]); area_b = (b[2]-b[0])*(b[3]-b[1])\n",
    "    return inter / (area_a + area_b - inter + 1e-6)\n",
    "\n",
    "def aabb_from_poly(poly):\n",
    "    xs = poly[:,0]; ys = poly[:,1]\n",
    "    return [int(xs.min()), int(ys.min()), int(xs.max()), int(ys.max())]\n",
    "\n",
    "def blur_polygon(img, poly, ksize=41, dilate_px=3):\n",
    "    # expand bbox slightly to be safe\n",
    "    aabb = aabb_from_poly(poly)\n",
    "    aabb = [max(0,aabb[0]-dilate_px), max(0,aabb[1]-dilate_px),\n",
    "            min(img.shape[1]-1, aabb[2]+dilate_px), min(img.shape[0]-1, aabb[3]+dilate_px)]\n",
    "    x0,y0,x1,y1 = aabb\n",
    "    if x1<=x0 or y1<=y0: return img\n",
    "    roi = img[y0:y1, x0:x1].copy()\n",
    "    img[y0:y1, x0:x1] = cv2.GaussianBlur(roi, (ksize,ksize), 0)\n",
    "    return img\n",
    "\n",
    "class Hysteresis:\n",
    "    def __init__(self, iou_thresh=0.3, k_confirm=2, k_hold=8):\n",
    "        self.iou_thresh = iou_thresh\n",
    "        self.k_confirm  = k_confirm\n",
    "        self.k_hold     = k_hold\n",
    "        self.tracks     = {}  # id -> dict\n",
    "        self.next_id    = 1\n",
    "        self.frame_id   = 0\n",
    "\n",
    "    def update(self, polys_active: List[Tuple[np.ndarray, bool]]):\n",
    "        self.frame_id += 1\n",
    "        # polys_active: list of (poly, is_positive_now)\n",
    "        # match by IoU against existing tracks\n",
    "        used = [False]*len(polys_active)\n",
    "        for tid,t in list(self.tracks.items()):\n",
    "            # find best match\n",
    "            best_j, best_iou = -1, 0.0\n",
    "            taabb = aabb_from_poly(t[\"poly\"])\n",
    "            for j,(poly,pos) in enumerate(polys_active):\n",
    "                if used[j]: continue\n",
    "                paabb = aabb_from_poly(poly)\n",
    "                ov = iou(taabb, paabb)\n",
    "                if ov > best_iou:\n",
    "                    best_iou, best_j = ov, j\n",
    "            if best_iou >= self.iou_thresh and best_j >= 0:\n",
    "                poly,pos = polys_active[best_j]\n",
    "                used[best_j] = True\n",
    "                t[\"poly\"] = poly\n",
    "                t[\"last\"] = self.frame_id\n",
    "                if pos:\n",
    "                    t[\"hits\"] += 1\n",
    "                    if not t[\"active\"] and t[\"hits\"] >= self.k_confirm:\n",
    "                        t[\"active\"] = True\n",
    "                # decay handled by time since last\n",
    "            # deactivate after hold window\n",
    "            if t[\"active\"] and (self.frame_id - t[\"last\"]) > self.k_hold:\n",
    "                t[\"active\"] = False\n",
    "\n",
    "        # new tracks\n",
    "        for j,(poly,pos) in enumerate(polys_active):\n",
    "            if not used[j]:\n",
    "                self.tracks[self.next_id] = {\n",
    "                    \"poly\": poly, \"hits\": 1 if pos else 0, \"active\": bool(pos) and (1>=self.k_confirm), \"last\": self.frame_id\n",
    "                }\n",
    "                self.next_id += 1\n",
    "\n",
    "        # GC stale\n",
    "        drop = [tid for tid,t in self.tracks.items() if (self.frame_id - t[\"last\"]) > (3*self.k_hold)]\n",
    "        for tid in drop: self.tracks.pop(tid, None)\n",
    "\n",
    "        return [(t[\"poly\"], t[\"active\"]) for t in self.tracks.values()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2c22d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame_bgr,\n",
    "                  conf_thresh=0.35,\n",
    "                  line_first=True,\n",
    "                  ksize=41,\n",
    "                  min_area=80):\n",
    "    \"\"\"\n",
    "    Returns a list of polygons to blur (in pixel coords) and the rendered frame.\n",
    "    \"\"\"\n",
    "    H,W = frame_bgr.shape[:2]\n",
    "    data = ocr.infer(frame_bgr)  # docTR-like export\n",
    "    pages = data.get(\"pages\", [])\n",
    "    if not pages: return frame_bgr, []\n",
    "\n",
    "    blur_polys = []\n",
    "\n",
    "    # 1) Try line-level PII: if the whole line looks like an address, blur all its words.\n",
    "    for blk in pages[0].get(\"blocks\", []):\n",
    "        for line in blk.get(\"lines\", []):\n",
    "            words = line.get(\"words\", [])\n",
    "            if not words: continue\n",
    "            line_text = \" \".join([w.get(\"value\",\"\") for w in words]).strip()\n",
    "            line_conf = np.mean([float(w.get(\"confidence\", 1.0)) for w in words]) if words else 1.0\n",
    "\n",
    "            word_polys = []\n",
    "            for w in words:\n",
    "                geom = w[\"geometry\"]\n",
    "                poly = poly_from_box_norm(geom, W, H)\n",
    "                if cv2.contourArea(poly) < min_area:\n",
    "                    continue\n",
    "                word_polys.append(poly)\n",
    "\n",
    "            if line_first and is_pii_text(line_text, line_conf, conf_thresh):\n",
    "                blur_polys.extend(word_polys)\n",
    "            else:\n",
    "                # 2) Otherwise test each word\n",
    "                for w,poly in zip(words, word_polys):\n",
    "                    if is_pii_text(w.get(\"value\",\"\"), float(w.get(\"confidence\",1.0)), conf_thresh):\n",
    "                        blur_polys.append(poly)\n",
    "\n",
    "    # Render (polys only; no stabilization here)\n",
    "    out = frame_bgr.copy()\n",
    "    for poly in blur_polys:\n",
    "        out = blur_polygon(out, poly, ksize=ksize, dilate_px=3)\n",
    "        cv2.polylines(out, [poly], True, (0,255,0), 2)\n",
    "    return out, blur_polys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b9a657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_live(source=0, width=1280, height=720, fps_target=30,\n",
    "             conf_thresh=0.35, k_confirm=2, k_hold=8, ksize=41):\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    if width:  cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "    if height: cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "    if fps_target: cap.set(cv2.CAP_PROP_FPS, fps_target)\n",
    "    assert cap.isOpened(), \"Cannot open video source.\"\n",
    "\n",
    "    stab = Hysteresis(iou_thresh=0.3, k_confirm=k_confirm, k_hold=k_hold)\n",
    "\n",
    "    while True:\n",
    "        t0 = time.time()\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "\n",
    "        frame_proc, polys = process_frame(frame, conf_thresh=conf_thresh, line_first=True, ksize=ksize)\n",
    "        # convert to (poly, pos) for stabilization\n",
    "        st = stab.update([(p, True) for p in polys])\n",
    "\n",
    "        out = frame.copy()\n",
    "        for poly, active in st:\n",
    "            if active:\n",
    "                out = blur_polygon(out, poly, ksize=ksize, dilate_px=3)\n",
    "                cv2.polylines(out, [poly], True, (0,255,0), 2)\n",
    "        fps = 1.0 / max(time.time()-t0, 1e-3)\n",
    "        cv2.putText(out, f\"DBNet+PARSeq | FPS {fps:.1f}\", (10,28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n",
    "        cv2.imshow(\"Live PII Blur (ESC to quit)\", out)\n",
    "        if cv2.waitKey(1) & 0xFF == 27: break\n",
    "\n",
    "    cap.release(); cv2.destroyAllWindows()\n",
    "\n",
    "# To run:\n",
    "# run_live(source=0)   # ESC to exit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a79a9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_video_file(path, out_path=\"output_blurred.mp4\",\n",
    "                   conf_thresh=0.35, k_confirm=2, k_hold=8, ksize=41):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    assert cap.isOpened(), f\"Cannot open {path}\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    FPS = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "    writer = cv2.VideoWriter(out_path, fourcc, FPS, (W,H))\n",
    "    stab = Hysteresis(iou_thresh=0.3, k_confirm=k_confirm, k_hold=k_hold)\n",
    "\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "        _, polys = process_frame(frame, conf_thresh=conf_thresh, line_first=True, ksize=ksize)\n",
    "        st = stab.update([(p, True) for p in polys])\n",
    "\n",
    "        out = frame.copy()\n",
    "        for poly, active in st:\n",
    "            if active:\n",
    "                out = blur_polygon(out, poly, ksize=ksize, dilate_px=3)\n",
    "        writer.write(out)\n",
    "\n",
    "    writer.release(); cap.release()\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "# Example:\n",
    "# run_video_file(\"sample_street.mp4\", out_path=\"sample_street_blurred.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71a05b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blur *all* detected text (regardless of PII) for a quick demo:\n",
    "BLUR_ALL = False\n",
    "\n",
    "def process_frame_alltext(frame_bgr, ksize=41, min_area=80):\n",
    "    H,W = frame_bgr.shape[:2]\n",
    "    data = ocr.infer(frame_bgr)\n",
    "    pages = data.get(\"pages\", [])\n",
    "    blur_polys=[]\n",
    "    for blk in pages[0].get(\"blocks\", []):\n",
    "        for line in blk.get(\"lines\", []):\n",
    "            for w in line.get(\"words\", []):\n",
    "                poly = poly_from_box_norm(w[\"geometry\"], W, H)\n",
    "                if cv2.contourArea(poly) >= min_area:\n",
    "                    blur_polys.append(poly)\n",
    "    out = frame_bgr.copy()\n",
    "    for p in blur_polys:\n",
    "        out = blur_polygon(out, p, ksize=ksize, dilate_px=3)\n",
    "    return out, blur_polys\n",
    "\n",
    "# In run_* loops, swap process_frame → process_frame_alltext when BLUR_ALL=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70192c75",
   "metadata": {},
   "source": [
    "# PII Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c2d6e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives: 10699 Negatives: 8905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                      text  label          type\n",
       " 0                            Tiburce Evans      1  NAME_STUDENT\n",
       " 1  https://www.instagram.com/tiburce-evans      1  URL_PERSONAL\n",
       " 2                                 bLBeoRIe      1        ID_NUM\n",
       " 3                    001-691-518-9820x5621      1     PHONE_NUM\n",
       " 5                       Rose-Mai Rodriguez      1  NAME_STUDENT,\n",
       "                                                 text  label type\n",
       " 0                                    applications of      0    O\n",
       " 1                               inclusivity criteria      0    O\n",
       " 2  from this experience can inform future applica...      0    O\n",
       " 3                                             Design      0    O\n",
       " 4                                                 of      0    O)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, re, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_FILE = Path(\"../datasets/mixtral_pii/mixtral-8x7b-v1.json\")  # path to the JSON holding 'root': [{...}]\n",
    "\n",
    "def join_tokens(tokens, trailing_ws):\n",
    "    \"\"\"Rebuilds contiguous text the way your dataset intended (space only if trailing_ws[i] is True).\"\"\"\n",
    "    out = []\n",
    "    for t, tw in zip(tokens, trailing_ws):\n",
    "        out.append(t)\n",
    "        if tw: out.append(\" \")\n",
    "    return \"\".join(out).rstrip()\n",
    "\n",
    "def extract_entities(tokens, labels, trailing_ws):\n",
    "    \"\"\"\n",
    "    From BIO labels, build a list of entity dicts:\n",
    "    [{'text': '...', 'type': 'NAME_STUDENT', 'span': (i0, i1)}]\n",
    "    Also returns a list of contiguous O-spans: [(start, end), ...]\n",
    "    \"\"\"\n",
    "    ents = []\n",
    "    i = 0\n",
    "    n = len(tokens)\n",
    "    o_spans = []\n",
    "    # collect contiguous O runs for negative sampling\n",
    "    while i < n:\n",
    "        lab = labels[i]\n",
    "        if lab == \"O\":\n",
    "            j = i\n",
    "            while j < n and labels[j] == \"O\":\n",
    "                j += 1\n",
    "            o_spans.append((i, j))   # [i, j)\n",
    "            i = j\n",
    "        else:\n",
    "            # expecting 'B-XXX' then possibly several 'I-XXX'\n",
    "            if not lab.startswith(\"B-\"):\n",
    "                # if broken BIO, treat as B-\n",
    "                typ = lab.split(\"-\", 1)[-1] if \"-\" in lab else lab\n",
    "            else:\n",
    "                typ = lab[2:]\n",
    "            j = i + 1\n",
    "            while j < n and labels[j].startswith(\"I-\") and labels[j][2:] == typ:\n",
    "                j += 1\n",
    "            # rebuild entity string with dataset's spacing\n",
    "            text_parts = []\n",
    "            for k in range(i, j):\n",
    "                text_parts.append(tokens[k])\n",
    "                if trailing_ws[k]:\n",
    "                    text_parts.append(\" \")\n",
    "            ent_text = \"\".join(text_parts).strip()\n",
    "            if ent_text:\n",
    "                ents.append({\"text\": ent_text, \"type\": typ, \"span\": (i, j)})\n",
    "            i = j\n",
    "    return ents, o_spans\n",
    "\n",
    "def sample_negatives(tokens, trailing_ws, o_spans, pos_len_hist, max_neg=20000):\n",
    "    \"\"\"\n",
    "    Sample negative strings from O regions, with a length distribution similar to positives.\n",
    "    pos_len_hist: dict {length_in_tokens: count}\n",
    "    \"\"\"\n",
    "    negatives = []\n",
    "    # Flatten a list of candidate (start,end) windows with lengths we need\n",
    "    lens = []\n",
    "    for L, cnt in pos_len_hist.items():\n",
    "        lens += [L] * cnt\n",
    "    random.shuffle(lens)\n",
    "    for L in lens:\n",
    "        # pick a random O-span that can fit length L\n",
    "        candidates = [(s, e) for (s, e) in o_spans if (e - s) >= L]\n",
    "        if not candidates: \n",
    "            continue\n",
    "        s, e = random.choice(candidates)\n",
    "        start = random.randint(s, e - L)\n",
    "        end = start + L\n",
    "        # build text\n",
    "        text_parts = []\n",
    "        for k in range(start, end):\n",
    "            text_parts.append(tokens[k])\n",
    "            if trailing_ws[k]:\n",
    "                text_parts.append(\" \")\n",
    "        neg = \"\".join(text_parts).strip()\n",
    "        if len(neg) >= 2 and not all(ch in \",.;:!?-–—()[]{}'\\\"/\\\\|\" for ch in neg):\n",
    "            negatives.append({\"text\": neg, \"type\": \"O\"})\n",
    "        if len(negatives) >= max_neg:\n",
    "            break\n",
    "    return negatives\n",
    "\n",
    "def load_bio_json(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "    docs = raw[\"root\"] if \"root\" in raw else raw\n",
    "    all_pos = []\n",
    "    all_neg = []\n",
    "    for d in docs:\n",
    "        tokens = d[\"tokens\"]\n",
    "        labels = d[\"labels\"]\n",
    "        trailing = d.get(\"trailing_whitespace\", [True]*len(tokens))\n",
    "        ents, o_spans = extract_entities(tokens, labels, trailing)\n",
    "        # build positive length histogram\n",
    "        len_hist = {}\n",
    "        for ent in ents:\n",
    "            L = ent[\"span\"][1] - ent[\"span\"][0]\n",
    "            len_hist[L] = len_hist.get(L, 0) + 1\n",
    "        negs = sample_negatives(tokens, trailing, o_spans, len_hist, max_neg=5_000)  # cap optional\n",
    "        all_pos.extend(ents)\n",
    "        all_neg.extend(negs)\n",
    "    # Deduplicate texts (optional)\n",
    "    pos_df = pd.DataFrame([{\"text\": e[\"text\"], \"label\": 1, \"type\": e[\"type\"]} for e in all_pos]).drop_duplicates(\"text\")\n",
    "    neg_df = pd.DataFrame([{\"text\": e[\"text\"], \"label\": 0, \"type\": e[\"type\"]} for e in all_neg]).drop_duplicates(\"text\")\n",
    "    return pos_df, neg_df\n",
    "\n",
    "# Load\n",
    "pos_df, neg_df = load_bio_json(DATA_FILE)\n",
    "print(\"Positives:\", len(pos_df), \"Negatives:\", len(neg_df))\n",
    "pos_df.head(), neg_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ac1737f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15683, 3921, np.float64(0.5457501753491041), np.float64(0.5457791379750063))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Balance classes (optional): subsample negatives to match positives × factor\n",
    "NEG_FACTOR = 1.5\n",
    "neg_sample = neg_df.sample(\n",
    "    n=min(len(neg_df), int(NEG_FACTOR * len(pos_df))),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df = pd.concat([pos_df[[\"text\",\"label\"]], neg_sample[[\"text\",\"label\"]]], ignore_index=True)\n",
    "df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)  # shuffle\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "len(X_train), len(X_val), y_train.mean(), y_val.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e2d1884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: {'thr': 0.6000000000000001, 'p': 0.9712696941612604, 'r': 0.9794392523364486, 'f1': 0.9753373662168451}\n",
      "Saved → pii_clf.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np, joblib\n",
    "\n",
    "vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=2)\n",
    "Xtr = vec.fit_transform(X_train)\n",
    "Xva = vec.transform(X_val)\n",
    "\n",
    "classes = np.array([0,1])\n",
    "cw = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "clf = LogisticRegression(solver=\"liblinear\", max_iter=1000, class_weight={0:cw[0],1:cw[1]})\n",
    "clf.fit(Xtr, y_train)\n",
    "\n",
    "proba_val = clf.predict_proba(Xva)[:,1]\n",
    "\n",
    "def sweep_threshold(y_true, proba, lo=0.2, hi=0.9, steps=36, prefer=\"f1\"):\n",
    "    best = {\"thr\":0.5,\"p\":0,\"r\":0,\"f1\":0}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        yhat = (proba >= thr).astype(int)\n",
    "        p,r,f1,_ = precision_recall_fscore_support(y_true, yhat, average=\"binary\", zero_division=0)\n",
    "        if ({\"precision\":p,\"recall\":r,\"f1\":f1}[prefer]) > best[prefer]:\n",
    "            best = {\"thr\":float(thr),\"p\":float(p),\"r\":float(r),\"f1\":float(f1)}\n",
    "    return best\n",
    "\n",
    "best = sweep_threshold(y_val.values, proba_val, prefer=\"f1\")\n",
    "print(\"Best threshold:\", best)\n",
    "\n",
    "joblib.dump({\"vec\":vec,\"clf\":clf,\"thr\":best[\"thr\"]}, \"pii_clf.joblib\")\n",
    "print(\"Saved → pii_clf.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d20bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal rules — keep your stronger rules if you already have them\n",
    "STREET_TOKENS = r\"(Street|St|Road|Rd|Avenue|Ave|Drive|Dr|Lane|Ln|Boulevard|Blvd|Way|Terrace|Ter|Court|Ct|Crescent|Cres|Place|Pl|Highway|Hwy|Expressway|Expwy|Jalan|Jln|Lorong|Lor)\"\n",
    "UNIT          = r\"#\\s?\\d{1,3}-\\d{1,4}\"\n",
    "POSTAL_SG     = r\"\\b(?:S\\s*)?\\d{6}\\b\"\n",
    "HOUSE_NO      = r\"\\b(?:Blk|Block)?\\s?\\d{1,5}[A-Z]?\\b\"\n",
    "COMPOSED      = rf\"{HOUSE_NO}.*\\b{STREET_TOKENS}\\b\"\n",
    "REGEXES  = [re.compile(p, re.IGNORECASE) for p in [STREET_TOKENS, UNIT, POSTAL_SG, COMPOSED]]\n",
    "\n",
    "def rule_is_pii(text: str) -> int:\n",
    "    if not text: return 0\n",
    "    t = text.strip()\n",
    "    return int(any(rx.search(t) for rx in REGEXES))\n",
    "\n",
    "# Load classifier once at startup\n",
    "_bundle = joblib.load(\"pii_clf.joblib\")\n",
    "_VEC, _CLF, _THR = _bundle[\"vec\"], _bundle[\"clf\"], float(_bundle[\"thr\"])\n",
    "\n",
    "def pii_prob(text: str) -> float:\n",
    "    if not text: return 0.0\n",
    "    return float(_CLF.predict_proba(_VEC.transform([text]))[0,1])\n",
    "\n",
    "def is_pii_hybrid(text: str, conf: float, conf_thresh=0.35) -> bool:\n",
    "    if not text or conf < conf_thresh: return False\n",
    "    if rule_is_pii(text):             # strong rule short-circuit\n",
    "        return True\n",
    "    return pii_prob(text) >= _THR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5d91b",
   "metadata": {},
   "source": [
    "# Fine-tuned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d8d5527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR predictor ready.\n"
     ]
    }
   ],
   "source": [
    "from doctr.models import ocr_predictor\n",
    "ocr = ocr_predictor(det_arch=\"db_resnet50\", reco_arch=\"parseq\", pretrained=True).to(device).eval()\n",
    "print(\"OCR predictor ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40b2baf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier loaded with threshold = 0.6000000000000001\n"
     ]
    }
   ],
   "source": [
    "# Load classifier bundle saved earlier\n",
    "bundle = joblib.load(\"pii_clf.joblib\")\n",
    "VEC, CLF, THR = bundle[\"vec\"], bundle[\"clf\"], float(bundle[\"thr\"])\n",
    "print(\"Classifier loaded with threshold =\", THR)\n",
    "\n",
    "# Rules (keep/extend as needed for your locale)\n",
    "STREET_TOKENS = r\"(Street|St|Road|Rd|Avenue|Ave|Drive|Dr|Lane|Ln|Boulevard|Blvd|Way|Terrace|Ter|Court|Ct|Crescent|Cres|Place|Pl|Highway|Hwy|Expressway|Expwy|Jalan|Jln|Lorong|Lor)\"\n",
    "UNIT          = r\"#\\s?\\d{1,3}-\\d{1,4}\"\n",
    "POSTAL_SG     = r\"\\b(?:S\\s*)?\\d{6}\\b\"\n",
    "HOUSE_NO      = r\"\\b(?:Blk|Block)?\\s?\\d{1,5}[A-Z]?\\b\"\n",
    "COMPOSED      = rf\"{HOUSE_NO}.*\\b{STREET_TOKENS}\\b\"\n",
    "REGEXES = [re.compile(p, re.I) for p in [STREET_TOKENS, UNIT, POSTAL_SG, COMPOSED]]\n",
    "\n",
    "def rule_is_pii(text: str) -> int:\n",
    "    if not text: return 0\n",
    "    t = text.strip()\n",
    "    return int(any(rx.search(t) for rx in REGEXES))\n",
    "\n",
    "def pii_prob(text: str) -> float:\n",
    "    if not text: return 0.0\n",
    "    return float(CLF.predict_proba(VEC.transform([text]))[0,1])\n",
    "\n",
    "def is_pii_hybrid(text: str, conf: float, conf_thresh: float = 0.35) -> bool:\n",
    "    \"\"\"Rules ∨ ML with a confidence gate from OCR.\"\"\"\n",
    "    if not text or conf < conf_thresh:\n",
    "        return False\n",
    "    if rule_is_pii(text):\n",
    "        return True\n",
    "    return pii_prob(text) >= THR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c4c1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_from_box_norm(box, W, H):\n",
    "    (x0,y0),(x1,y1) = box\n",
    "    x0, x1 = int(x0*W), int(x1*W)\n",
    "    y0, y1 = int(y0*H), int(y1*H)\n",
    "    return np.array([[x0,y0],[x1,y0],[x1,y1],[x0,y1]], dtype=np.int32)\n",
    "\n",
    "def aabb(poly):\n",
    "    xs, ys = poly[:,0], poly[:,1]\n",
    "    return [int(xs.min()), int(ys.min()), int(xs.max()), int(ys.max())]\n",
    "\n",
    "def iou(a, b):\n",
    "    xi1, yi1 = max(a[0],b[0]), max(a[1],b[1])\n",
    "    xi2, yi2 = min(a[2],b[2]), min(a[3],b[3])\n",
    "    inter = max(0, xi2-xi1) * max(0, yi2-yi1)\n",
    "    if inter <= 0: return 0.0\n",
    "    area_a = (a[2]-a[0])*(a[3]-a[1]); area_b = (b[2]-b[0])*(b[3]-b[1])\n",
    "    return inter / (area_a + area_b - inter + 1e-6)\n",
    "\n",
    "def blur_polygon(img, poly, ksize=41, pad=3):\n",
    "    x0,y0,x1,y1 = aabb(poly)\n",
    "    x0, y0 = max(0, x0-pad), max(0, y0-pad)\n",
    "    x1, y1 = min(img.shape[1]-1, x1+pad), min(img.shape[0]-1, y1+pad)\n",
    "    if x1<=x0 or y1<=y0: return img\n",
    "    roi = img[y0:y1, x0:x1].copy()\n",
    "    img[y0:y1, x0:x1] = cv2.GaussianBlur(roi, (ksize, ksize), 0)\n",
    "    return img\n",
    "\n",
    "class Hysteresis:\n",
    "    \"\"\"Confirm after K_confirm hits; hold for K_hold frames.\"\"\"\n",
    "    def __init__(self, iou_thresh=0.3, K_confirm=2, K_hold=8):\n",
    "        self.iou_thresh, self.K_confirm, self.K_hold = iou_thresh, K_confirm, K_hold\n",
    "        self.tracks = {}; self.next_id=1; self.frame=0\n",
    "    def update(self, polys):\n",
    "        self.frame += 1\n",
    "        used = [False]*len(polys)\n",
    "        # match\n",
    "        for tid,t in list(self.tracks.items()):\n",
    "            ta = aabb(t[\"poly\"])\n",
    "            best, bj = 0.0, -1\n",
    "            for j,p in enumerate(polys):\n",
    "                if used[j]: continue\n",
    "                ov = iou(ta, aabb(p))\n",
    "                if ov > best: best, bj = ov, j\n",
    "            if best >= self.iou_thresh and bj>=0:\n",
    "                t[\"poly\"] = polys[bj]; t[\"hits\"] += 1; t[\"last\"]=self.frame\n",
    "                if not t[\"active\"] and t[\"hits\"] >= self.K_confirm: t[\"active\"]=True\n",
    "                used[bj] = True\n",
    "            if t[\"active\"] and (self.frame - t[\"last\"]) > self.K_hold: t[\"active\"]=False\n",
    "        # new\n",
    "        for j,p in enumerate(polys):\n",
    "            if not used[j]:\n",
    "                self.tracks[self.next_id] = {\"poly\":p,\"hits\":1,\"active\":(1>=self.K_confirm),\"last\":self.frame}\n",
    "                self.next_id += 1\n",
    "        # gc\n",
    "        drop=[tid for tid,t in self.tracks.items() if (self.frame - t[\"last\"]) > (3*self.K_hold)]\n",
    "        for tid in drop: self.tracks.pop(tid, None)\n",
    "        return [(t[\"poly\"], t[\"active\"]) for t in self.tracks.values()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f734e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_pii_polys(frame_bgr, conf_thresh=0.35, min_area=80):\n",
    "    H,W = frame_bgr.shape[:2]\n",
    "    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    doc = ocr([rgb]).export()\n",
    "    polys_to_blur = []\n",
    "\n",
    "    for blk in doc[\"pages\"][0].get(\"blocks\", []):\n",
    "        for line in blk.get(\"lines\", []):\n",
    "            # word-level decisions\n",
    "            for w in line.get(\"words\", []):\n",
    "                text = w.get(\"value\",\"\")\n",
    "                conf = float(w.get(\"confidence\", 1.0))\n",
    "                poly = poly_from_box_norm(w[\"geometry\"], W, H)\n",
    "                if cv2.contourArea(poly) < min_area: \n",
    "                    continue\n",
    "                if is_pii_hybrid(text, conf, conf_thresh):\n",
    "                    polys_to_blur.append(poly)\n",
    "    return polys_to_blur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0290a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_video_with_classifier(\n",
    "    src_path,\n",
    "    out_path=\"output_blurred.mp4\",\n",
    "    conf_thresh=0.35,\n",
    "    K_confirm=2, K_hold=8,\n",
    "    ksize=41\n",
    "):\n",
    "    cap = cv2.VideoCapture(str(src_path))\n",
    "    assert cap.isOpened(), f\"Cannot open {src_path}\"\n",
    "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH));  H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    FPS = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(str(out_path), fourcc, FPS, (W,H))\n",
    "    stab = Hysteresis(iou_thresh=0.3, K_confirm=K_confirm, K_hold=K_hold)\n",
    "\n",
    "    frame_idx=0\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "        frame_idx += 1\n",
    "\n",
    "        # 1) Collect PII polygons from this frame\n",
    "        polys = collect_pii_polys(frame, conf_thresh=conf_thresh)\n",
    "        # 2) Stabilize (confirm/hold)\n",
    "        stabilized = stab.update(polys)\n",
    "\n",
    "        # 3) Blur active polygons\n",
    "        out = frame.copy()\n",
    "        for poly, active in stabilized:\n",
    "            if active:\n",
    "                out = blur_polygon(out, poly, ksize=ksize, pad=3)\n",
    "\n",
    "        writer.write(out)\n",
    "        if frame_idx % 30 == 0:\n",
    "            print(f\"Processed frame {frame_idx}\")\n",
    "\n",
    "    writer.release(); cap.release()\n",
    "    print(\"Saved →\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657d6ca6",
   "metadata": {},
   "source": [
    "## RUNNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a246da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_live(source=0,\n",
    "             width=1280, height=720,  # set None to keep native\n",
    "             conf_thresh=0.35,\n",
    "             K_confirm=2, K_hold=8,\n",
    "             ksize=41,\n",
    "             show_boxes=False,        # draw green boxes where we blur\n",
    "             target_fps_txt=True):    # draw FPS overlay\n",
    "    \"\"\"\n",
    "    Live PII blur from webcam/RTSP/USB camera.\n",
    "    - Press ESC to quit.\n",
    "    - source can be int (0,1,...) or a string (RTSP/URL).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    if width:  cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n",
    "    if height: cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 2)\n",
    "\n",
    "    assert cap.isOpened(), f\"Cannot open video source: {source}\"\n",
    "    stab = Hysteresis(iou_thresh=0.3, K_confirm=K_confirm, K_hold=K_hold)\n",
    "\n",
    "    print(\"Live stream started. Press ESC to exit.\")\n",
    "    while True:\n",
    "        t0 = time.time()\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        # Collect PII polygons for this frame (hybrid rules ∨ classifier)\n",
    "        polys = collect_pii_polys(frame, conf_thresh=conf_thresh)\n",
    "\n",
    "        # Stabilize detections (confirm/hold)\n",
    "        tracks = stab.update(polys)\n",
    "\n",
    "        # Blur active tracks\n",
    "        out = frame.copy()\n",
    "        active_cnt = 0\n",
    "        for poly, active in tracks:\n",
    "            if active:\n",
    "                active_cnt += 1\n",
    "                out = blur_polygon(out, poly, ksize=ksize, pad=3)\n",
    "                if show_boxes:\n",
    "                    cv2.polylines(out, [poly], True, (0,255,0), 2)\n",
    "\n",
    "        # HUD\n",
    "        if target_fps_txt:\n",
    "            fps = 1.0 / max(time.time() - t0, 1e-3)\n",
    "            cv2.putText(out, f\"DBNet+PARSeq | Hybrid PII | FPS {fps:.1f} | Blurs {active_cnt}\",\n",
    "                        (10,28), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255,255,255), 2)\n",
    "            cv2.putText(out, \"Privacy Filter ON\", (10,55),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (30,220,30), 2)\n",
    "\n",
    "        cv2.imshow(\"Live PII Blur (ESC to quit)\", out)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:  # ESC\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Live stream ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29387d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live stream started. Press ESC to exit.\n",
      "Live stream ended.\n"
     ]
    }
   ],
   "source": [
    "# Default webcam\n",
    "run_live(source=0, conf_thresh=0.35, K_confirm=2, K_hold=8, ksize=41, show_boxes=True)\n",
    "\n",
    "# If you have multiple cameras, try 1, 2, ...; for RTSP/USB string sources, pass the URL instead.\n",
    "# run_live(source=\"rtsp://user:pass@ip:554/stream1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff248b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
